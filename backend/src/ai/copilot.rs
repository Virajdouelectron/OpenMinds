use anyhow::{Context, Result};
use async_openai::{
    types::{
        ChatCompletionRequestMessage, ChatCompletionRequestMessageArgs,
        CreateChatCompletionRequestArgs, Role,
    },
    Client,
};
use serde::{Deserialize, Serialize};
use std::env;
use tokio::sync::OnceCell;

static OPENAI_CLIENT: OnceCell<Client> = OnceCell::const_new();

#[derive(Debug, Serialize, Deserialize)]
pub struct CodeSuggestion {
    pub code: String,
    pub language: String,
    pub explanation: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CodeAnalysis {
    pub suggestions: Vec<CodeSuggestion>,
    pub issues: Vec<String>,
    pub summary: String,
}

pub struct AICopilot {
    client: &'static Client,
    model: String,
}

impl AICopilot {
    pub async fn new() -> Result<Self> {
        // Initialize the OpenAI client if not already done
        let client = OPENAI_CLIENT
            .get_or_try_init(|| async {
                let api_key = env::var("OPENAI_API_KEY")
                    .context("OPENAI_API_KEY environment variable not set")?;
                Ok(Client::new().with_api_key(api_key))
            })
            .await?;

        Ok(Self {
            client,
            model: env::var("OPENAI_MODEL").unwrap_or_else(|_| "gpt-4".to_string()),
        })
    }

    pub async fn get_code_completion(
        &self,
        code: &str,
        language: &str,
        context: Option<&str>,
    ) -> Result<CodeSuggestion> {
        let system_prompt = format!(
            "You are an expert {language} programmer. Provide a concise and accurate code completion based on the context."
        );

        let user_content = if let Some(ctx) = context {
            format!("Context: {}\n\nCode to complete:\n```{language}\n{code}\n```")
        } else {
            format!("Complete the following {language} code:\n```{language}\n{code}\n```")
        };

        let messages = vec![
            ChatCompletionRequestMessageArgs::default()
                .role(Role::System)
                .content(system_prompt)
                .build()?,
            ChatCompletionRequestMessageArgs::default()
                .role(Role::User)
                .content(user_content)
                .build()?,
        ];

        self.chat_completion(messages).await
    }

    pub async fn analyze_code(
        &self,
        code: &str,
        language: &str,
    ) -> Result<CodeAnalysis> {
        let system_prompt = format!(
            "You are an expert {language} code reviewer. Analyze the code for issues, suggest improvements, and provide a summary."
        );

        let user_content = format!(
            "Analyze this {language} code:\n```{language}\n{code}\n```\n\nProvide:
1. Code improvements with explanations
2. Potential issues or bugs
3. A brief summary of the code's purpose and quality"
        );

        let messages = vec![
            ChatCompletionRequestMessageArgs::default()
                .role(Role::System)
                .content(system_prompt)
                .build()?,
            ChatCompletionRequestMessageArgs::default()
                .role(Role::User)
                .content(user_content)
                .build()?,
        ];

        let response = self.chat_completion(messages).await?;
        
        // Parse the response into a structured format
        // This is a simplified version - in a real implementation, you might want to use
        // function calling or a more sophisticated parsing approach
        Ok(CodeAnalysis {
            suggestions: vec![CodeSuggestion {
                code: response.code,
                language: language.to_string(),
                explanation: response.explanation,
            }],
            issues: vec![],
            summary: "Analysis complete".to_string(),
        })
    }

    async fn chat_completion(
        &self,
        messages: Vec<ChatCompletionRequestMessage>,
    ) -> Result<CodeSuggestion> {
        let request = CreateChatCompletionRequestArgs::default()
            .model(&self.model)
            .messages(messages)
            .temperature(0.2)
            .max_tokens(1000)
            .build()?;

        let response = self
            .client
            .chat()
            .create(request)
            .await
            .context("Failed to get response from OpenAI")?;

        let content = response
            .choices
            .first()
            .and_then(|c| c.message.content.clone())
            .unwrap_or_default();

        // In a real implementation, you would parse the response into a more structured format
        // For now, we'll just return the raw content
        Ok(CodeSuggestion {
            code: content,
            language: "python".to_string(),
            explanation: "Generated by AI Copilot".to_string(),
        })
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use dotenv::dotenv;
    use std::env;

    #[tokio::test]
    #[ignore = "requires OPENAI_API_KEY"]
    async fn test_code_completion() {
        dotenv().ok();
        
        let copilot = AICopilot::new().await.unwrap();
        let code = "def calculate_average(numbers):";
        
        let result = copilot
            .get_code_completion(code, "python", None)
            .await
            .unwrap();
            
        assert!(!result.code.is_empty());
        assert!(result.code.contains("return"));
    }
}
